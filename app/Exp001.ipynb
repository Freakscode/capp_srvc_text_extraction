{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Paso 2: Crear Directorios de Trabajo\n",
    "import os\n",
    "\n",
    "# Define las rutas\n",
    "pdfs_dir = 'documents'  # Carpeta que contiene los PDFs\n",
    "output_text_dir = 'textos'  # Carpeta para guardar los textos extraídos\n",
    "output_parquet_path = 'datos_procesados.parquet'  # Ruta para el archivo .parquet\n",
    "\n",
    "# Crea las carpetas si no existen\n",
    "os.makedirs(pdfs_dir, exist_ok=True)\n",
    "os.makedirs(output_text_dir, exist_ok=True)\n",
    "\n",
    "# Paso 3: Instalar Dependencias\n",
    "#!pip install pdfminer.six spacy pyspellchecker transformers torch pandas pyarrow tqdm\n",
    "\n",
    "# Paso 4: Descargar Modelos Necesarios\n",
    "import spacy\n",
    "\n",
    "# Descargar el modelo BERT en español (BETO)\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Usaremos el modelo BETO para mejor compatibilidad con español\n",
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
    "modelo = BertModel.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
    "\n",
    "# Paso 5: Definir Funciones de Procesamiento\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pdfminer.high_level import extract_text\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Verifica si CUDA está disponible y configura el dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Cargar el modelo de spaCy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Inicializar el corrector ortográfico para español\n",
    "spell = SpellChecker(language='es')\n",
    "\n",
    "# Cargar el tokenizador y el modelo BERT en español, y moverlo al dispositivo adecuado\n",
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
    "modelo = BertModel.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased').to(device)\n",
    "\n",
    "def extraer_texto_pdf(ruta_pdf: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrae texto de un archivo PDF.\n",
    "\n",
    "    Args:\n",
    "        ruta_pdf (str): Ruta al archivo PDF.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto extraído del PDF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        texto = extract_text(ruta_pdf)\n",
    "        return texto\n",
    "    except Exception as e:\n",
    "        print(f'Error extrayendo texto de {ruta_pdf}: {e}')\n",
    "        return ''\n",
    "\n",
    "def limpiar_y_normalizar_texto(texto: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia y normaliza el texto extraído.\n",
    "\n",
    "    Args:\n",
    "        texto (str): Texto bruto extraído del PDF.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto limpio y normalizado.\n",
    "    \"\"\"\n",
    "    texto = re.sub(r'\\s+', ' ', texto)  # Reemplaza múltiples espacios por uno solo\n",
    "    texto = texto.strip()\n",
    "    texto = unicodedata.normalize('NFC', texto)  # Normaliza caracteres Unicode\n",
    "    return texto\n",
    "\n",
    "def tokenizar_y_lemmatizar(texto: str, nlp) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokeniza y lematiza el texto utilizando spaCy.\n",
    "\n",
    "    Args:\n",
    "        texto (str): Texto limpio y normalizado.\n",
    "        nlp: Objeto de spaCy para procesamiento.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de tokens lematizados.\n",
    "    \"\"\"\n",
    "    doc = nlp(texto)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return tokens\n",
    "\n",
    "def corregir_ortografia(palabras: List[str], spell: SpellChecker) -> List[str]:\n",
    "    \"\"\"\n",
    "    Corrige la ortografía de una lista de palabras.\n",
    "\n",
    "    Args:\n",
    "        palabras (list): Lista de palabras lematizadas.\n",
    "        spell (SpellChecker): Objeto SpellChecker.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de palabras corregidas.\n",
    "    \"\"\"\n",
    "    palabras_corregidas = []\n",
    "    for palabra in palabras:\n",
    "        correccion = spell.correction(palabra)\n",
    "        if correccion:\n",
    "            if palabra.isupper():\n",
    "                correccion = correccion.upper()\n",
    "            elif palabra[0].isupper():\n",
    "                correccion = correccion.capitalize()\n",
    "            palabras_corregidas.append(correccion)\n",
    "        else:\n",
    "            palabras_corregidas.append(palabra)\n",
    "    return palabras_corregidas\n",
    "\n",
    "def generar_embeddings(texto: str, tokenizer, modelo, device) -> List[float]:\n",
    "    \"\"\"\n",
    "    Genera embeddings para el texto dado utilizando BERT en GPU si está disponible.\n",
    "\n",
    "    Args:\n",
    "        texto (str): Texto preprocesado.\n",
    "        tokenizer: Tokenizador de BERT.\n",
    "        modelo: Modelo de BERT.\n",
    "        device: Dispositivo a utilizar ('cuda' o 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        list: Embeddings generados por BERT como lista de floats.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(texto, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        salidas = modelo(**inputs)\n",
    "    embeddings = salidas.last_hidden_state.mean(dim=1)  # Promedio de las representaciones de tokens\n",
    "    return embeddings.squeeze().cpu().numpy().tolist()\n",
    "\n",
    "def procesar_pdf(ruta_pdf: str, nlp, spell, tokenizer, modelo, device) -> Dict:\n",
    "    \"\"\"\n",
    "    Procesa un documento PDF completo.\n",
    "\n",
    "    Args:\n",
    "        ruta_pdf (str): Ruta al archivo PDF.\n",
    "        nlp: Objeto de spaCy para procesamiento.\n",
    "        spell (SpellChecker): Objeto SpellChecker.\n",
    "        tokenizer: Tokenizador de BERT.\n",
    "        modelo: Modelo de BERT.\n",
    "        device: Dispositivo a utilizar ('cuda' o 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        dict: Información procesada del documento.\n",
    "    \"\"\"\n",
    "    nombre_archivo = os.path.basename(ruta_pdf)\n",
    "    texto = extraer_texto_pdf(ruta_pdf)\n",
    "    texto = limpiar_y_normalizar_texto(texto)\n",
    "    tokens = tokenizar_y_lemmatizar(texto, nlp)\n",
    "    palabras_corregidas = corregir_ortografia(tokens, spell)\n",
    "    texto_preprocesado = ' '.join(palabras_corregidas)\n",
    "    embeddings = generar_embeddings(texto_preprocesado, tokenizer, modelo, device)\n",
    "    \n",
    "    return {\n",
    "        \"nombre_archivo\": nombre_archivo,\n",
    "        \"ruta\": ruta_pdf,\n",
    "        \"embeddings\": embeddings,\n",
    "        \"texto\": texto_preprocesado\n",
    "    }\n",
    "\n",
    "def guardar_parquet(datos: List[Dict], ruta_parquet: str):\n",
    "    \"\"\"\n",
    "    Guarda los datos procesados en un archivo .parquet.\n",
    "\n",
    "    Args:\n",
    "        datos (list): Lista de diccionarios con la información procesada.\n",
    "        ruta_parquet (str): Ruta donde se guardará el archivo .parquet.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(datos)\n",
    "    # Selecciona solo las columnas necesarias\n",
    "    df = df[['nombre_archivo', 'ruta', 'embeddings']]\n",
    "    df.to_parquet(ruta_parquet, index=False)\n",
    "\n",
    "def guardar_texto(datos: List[Dict], carpeta_texto: str, formato: str = 'txt'):\n",
    "    \"\"\"\n",
    "    Guarda el texto extraído en archivos de texto plano o JSON.\n",
    "\n",
    "    Args:\n",
    "        datos (list): Lista de diccionarios con la información procesada.\n",
    "        carpeta_texto (str): Carpeta donde se guardarán los archivos de texto.\n",
    "        formato (str): Formato de los archivos de salida ('txt' o 'json').\n",
    "    \"\"\"\n",
    "    os.makedirs(carpeta_texto, exist_ok=True)\n",
    "    for dato in datos:\n",
    "        nombre_archivo = dato['nombre_archivo']\n",
    "        texto = dato['texto']\n",
    "        nombre_sin_ext = os.path.splitext(nombre_archivo)[0]\n",
    "        if formato == 'txt':\n",
    "            ruta_salida = os.path.join(carpeta_texto, f\"{nombre_sin_ext}.txt\")\n",
    "            with open(ruta_salida, 'w', encoding='utf-8') as f:\n",
    "                f.write(texto)\n",
    "        elif formato == 'json':\n",
    "            ruta_salida = os.path.join(carpeta_texto, f\"{nombre_sin_ext}.json\")\n",
    "            with open(ruta_salida, 'w', encoding='utf-8') as f:\n",
    "                json.dump({\"texto\": texto}, f, ensure_ascii=False, indent=4)\n",
    "        else:\n",
    "            print(f\"Formato {formato} no soportado. Skipping.\")\n",
    "\n",
    "def main(carpeta_pdfs: str, carpeta_salida_texto: str, ruta_parquet: str, formato_texto: str = 'txt'):\n",
    "    \"\"\"\n",
    "    Función principal para procesar todos los PDFs en una carpeta.\n",
    "\n",
    "    Args:\n",
    "        carpeta_pdfs (str): Ruta a la carpeta que contiene los archivos PDF.\n",
    "        carpeta_salida_texto (str): Ruta a la carpeta donde se guardarán los archivos de texto.\n",
    "        ruta_parquet (str): Ruta donde se guardará el archivo .parquet.\n",
    "        formato_texto (str): Formato de los archivos de salida de texto ('txt' o 'json').\n",
    "    \"\"\"\n",
    "    # Lista para almacenar los datos procesados\n",
    "    datos_procesados = []\n",
    "\n",
    "    # Obtener la lista de archivos PDF\n",
    "    pdfs = list(Path(carpeta_pdfs).glob('*.pdf'))\n",
    "    for ruta_pdf in tqdm(pdfs, desc=\"Procesando PDFs\"):\n",
    "        dato = procesar_pdf(str(ruta_pdf), nlp, spell, tokenizer, modelo, device)\n",
    "        datos_procesados.append(dato)\n",
    "        # Guardar texto inmediatamente para ahorrar memoria\n",
    "        guardar_texto([dato], carpeta_salida_texto, formato=formato_texto)\n",
    "    \n",
    "    # Guardar embeddings en .parquet\n",
    "    guardar_parquet(datos_procesados, ruta_parquet)\n",
    "    print(f\"Datos guardados en {ruta_parquet}\")\n",
    "    print(f\"Textos guardados en {carpeta_salida_texto} en formato {formato_texto}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando PDFs: 100%|██████████| 33/33 [2:12:53<00:00, 241.63s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos guardados en datos_procesados.parquet\n",
      "Textos guardados en textos en formato json\n"
     ]
    }
   ],
   "source": [
    "main(\n",
    "    carpeta_pdfs=pdfs_dir,\n",
    "    carpeta_salida_texto=output_text_dir,\n",
    "    ruta_parquet=output_parquet_path,\n",
    "    formato_texto='json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (33, 3)\n",
      "┌───────────────────────────────────┬───────────────────────────────────┬──────────────────────────┐\n",
      "│ nombre_archivo                    ┆ ruta                              ┆ embeddings               │\n",
      "│ ---                               ┆ ---                               ┆ ---                      │\n",
      "│ str                               ┆ str                               ┆ list[f64]                │\n",
      "╞═══════════════════════════════════╪═══════════════════════════════════╪══════════════════════════╡\n",
      "│ DETERMINACION DE LA VARIACION …   ┆ documents\\DETERMINACION DE LA …   ┆ [-0.084382, -0.119554, … │\n",
      "│                                   ┆                                   ┆ -0.17…                   │\n",
      "│ DISEÑO DE MODULO DIDACTICO DE …   ┆ documents\\DISEÑO DE MODULO DID…   ┆ [-0.211153, -0.224628, … │\n",
      "│                                   ┆                                   ┆ -0.15…                   │\n",
      "│ PROYECTO GRADO SERGIO Y ANGEL …   ┆ documents\\PROYECTO GRADO SERGI…   ┆ [-0.132736, 0.104929, …  │\n",
      "│                                   ┆                                   ┆ -0.477…                  │\n",
      "│ Rep-IUPB-Ing-Mec_Alimentación.…   ┆ documents\\Rep-IUPB-Ing-Mec_Ali…   ┆ [-0.230803, -0.04034, …  │\n",
      "│                                   ┆                                   ┆ -0.255…                  │\n",
      "│ Rep_IUPB_Ing-Mec_Reconversión.…   ┆ documents\\Rep_IUPB_Ing-Mec_Rec…   ┆ [-0.088525, -0.264471, … │\n",
      "│                                   ┆                                   ┆ -0.24…                   │\n",
      "│ …                                 ┆ …                                 ┆ …                        │\n",
      "│ Rep_IUPB_Ing_Mec_Postobón.pdf     ┆ documents\\Rep_IUPB_Ing_Mec_Pos…   ┆ [-0.151639, -0.305628, … │\n",
      "│                                   ┆                                   ┆ -0.13…                   │\n",
      "│ Rep_IUPB_Ing_Mec_Redeiseño.pdf    ┆ documents\\Rep_IUPB_Ing_Mec_Red…   ┆ [-0.186569, -0.056637, … │\n",
      "│                                   ┆                                   ┆ -0.24…                   │\n",
      "│ Rep_IUPB_Ing_Mec_Selladora.pdf    ┆ documents\\Rep_IUPB_Ing_Mec_Sel…   ┆ [-0.292773, -0.192642, … │\n",
      "│                                   ┆                                   ┆ -0.21…                   │\n",
      "│ Rep_IUPB_Ing_Mec_sistema-refri…   ┆ documents\\Rep_IUPB_Ing_Mec_sis…   ┆ [-0.222209, -0.022799, … │\n",
      "│                                   ┆                                   ┆ -0.10…                   │\n",
      "│ Rep_IUPB_Ing_Mec_Soporte.pdf      ┆ documents\\Rep_IUPB_Ing_Mec_Sop…   ┆ [-0.050519, -0.02872, …  │\n",
      "│                                   ┆                                   ┆ -0.348…                  │\n",
      "└───────────────────────────────────┴───────────────────────────────────┴──────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.read_parquet('datos_procesados.parquet')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La longitud del texto es: 52095\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Cargar el archivo JSON\n",
    "with open('textos/DETERMINACION DE LA VARIACION DEL RENDIMIENTO ENERGETICO DE UN AIRE ACONDICIONADO TIPO MINI SPLIT.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Obtener el texto del JSON\n",
    "texto = data.get('texto', '')\n",
    "\n",
    "# Calcular la longitud del texto\n",
    "longitud_texto = len(texto)\n",
    "print(f\"La longitud del texto es: {longitud_texto}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "# Reemplaza 'your_huggingface_token' con tu token de acceso personal de HuggingFace\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73cb0c37626341f1956776466f2eaed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Verificar si hay una GPU disponible y seleccionar el dispositivo adecuado\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Cargar el modelo y el tokenizador\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "llama_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\").to(device)\n",
    "\n",
    "def extraer_informacion(texto: str, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Extrae entidades reconocidas, resumen corto, palabras clave y tono de escritura de un texto dado.\n",
    "\n",
    "    Args:\n",
    "        texto (str): Texto del cual extraer la información.\n",
    "        tokenizer: Tokenizador del modelo.\n",
    "        model: Modelo de lenguaje.\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario con las entidades reconocidas, resumen corto, palabras clave y tono de escritura.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(texto, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    outputs = model.generate(**inputs, max_length=20000)\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Aquí se asume que el modelo devuelve la información en un formato estructurado\n",
    "    # como JSON o texto delimitado. Ajusta según sea necesario.\n",
    "    info = json.loads(decoded_output)\n",
    "    \n",
    "    return {\n",
    "        \"entidades_reconocidas\": info.get(\"entidades_reconocidas\", []),\n",
    "        \"resumen_corto\": info.get(\"resumen_corto\", \"\"),\n",
    "        \"palabras_clave\": info.get(\"palabras_clave\", []),\n",
    "        \"tono_de_escritura\": info.get(\"tono_de_escritura\", \"\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Definir la función para procesar un archivo JSON\n",
    "def procesar_archivo_json(ruta_json: str, tokenizer, model):\n",
    "    with open(ruta_json, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    texto = data.get('texto', '')\n",
    "    informacion_extraida = extraer_informacion(texto, tokenizer, model)\n",
    "    return informacion_extraida\n",
    "\n",
    "# Obtener la lista de archivos JSON en la carpeta 'textos'\n",
    "carpeta_textos = Path(output_text_dir)\n",
    "archivos_json = list(carpeta_textos.glob('*.json'))\n",
    "\n",
    "# Lista para almacenar los resultados\n",
    "resultados = []\n",
    "\n",
    "# Procesar cada archivo JSON\n",
    "for archivo_json in archivos_json:\n",
    "    informacion = procesar_archivo_json(str(archivo_json), llama_tokenizer, llama_model)\n",
    "    informacion['nombre_archivo'] = archivo_json.name\n",
    "    resultados.append(informacion)\n",
    "\n",
    "# Crear un dataframe de Polars con los resultados\n",
    "df_resultados = pl.DataFrame(resultados)\n",
    "print(df_resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "df = pl.read_parquet('datos_procesados.parquet')\n",
    "print(len(df[0,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, MilvusClient\n",
    "\n",
    "client = MilvusClient(\"milvus_demo.db\")\n",
    "client.create_collection(\n",
    "    collection_name = \"document_collection\",\n",
    "    dimension = 768\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
